{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N22Uz-kLiZW"
      },
      "source": [
        "**Main imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK1Jl7nkLnPA"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as disp\n",
        "from torch.nn.utils import spectral_norm    \n",
        "import os\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device.type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run1dh_hM0oO"
      },
      "source": [
        "**Import dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK383zeDM4Ac",
        "outputId": "8a35453f-df4e-4cb6-c791-f88d713097c4"
      },
      "outputs": [],
      "source": [
        "# helper function to make getting another batch of data easier\n",
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for x in iterable:\n",
        "            yield x\n",
        "\n",
        "class_names = ['apple','aquarium_fish','baby','bear','beaver','bed','bee','beetle','bicycle','bottle','bowl','boy','bridge','bus','butterfly','camel','can','castle','caterpillar','cattle','chair','chimpanzee','clock','cloud','cockroach','couch','crab','crocodile','cup','dinosaur','dolphin','elephant','flatfish','forest','fox','girl','hamster','house','kangaroo','computer_keyboard','lamp','lawn_mower','leopard','lion','lizard','lobster','man','maple_tree','motorcycle','mountain','mouse','mushroom','oak_tree','orange','orchid','otter','palm_tree','pear','pickup_truck','pine_tree','plain','plate','poppy','porcupine','possum','rabbit','raccoon','ray','road','rocket','rose','sea','seal','shark','shrew','skunk','skyscraper','snail','snake','spider','squirrel','streetcar','sunflower','sweet_pepper','table','tank','telephone','television','tiger','tractor','train','trout','tulip','turtle','wardrobe','whale','willow_tree','wolf','woman','worm',]\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.CIFAR100('data', train=True, download=True, transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])),\n",
        "    batch_size=64, drop_last=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.CIFAR100('data', train=False, download=True, transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize([32,32]),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])),\n",
        "    batch_size=64, drop_last=True)\n",
        "\n",
        "train_iterator = iter(cycle(train_loader))\n",
        "test_iterator = iter(cycle(test_loader))\n",
        "\n",
        "print(f'> Size of training dataset {len(train_loader.dataset)}')\n",
        "print(f'> Size of test dataset {len(test_loader.dataset)}')\n",
        "print(\"Number of classes: \", len(class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-FdW5HnimG2"
      },
      "source": [
        "**View some of the test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "BtJs-qxHRLXz",
        "outputId": "90986b66-733e-41fc-d01f-ddc442c4423e"
      },
      "outputs": [],
      "source": [
        "# let's view some of the training data\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "x, t = next(train_iterator)\n",
        "\n",
        "# Ensure the tensor is correctly moved to the GPU\n",
        "x = x.to(device)\n",
        "t = t.to(device)\n",
        "\n",
        "# Plot the images\n",
        "plt.imshow(torchvision.utils.make_grid(x).cpu().numpy().transpose(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generator, Discriminator, and Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = train_loader.batch_size\n",
        "# channels = 3\n",
        "# latent_dim = 256\n",
        "# image_size = 32\n",
        "\n",
        "num_classes = 100\n",
        "\n",
        "check_interval = 5\n",
        "\n",
        "num_of_epochs = 50000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Repo: https://github.com/atapour/dl-pytorch/blob/main/Conditional_GAN_Example/Conditional_GAN_Example.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A few parameters:\n",
        "n_channels = 3\n",
        "img_width = 32\n",
        "\n",
        "# define the generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_size=100, label_size=100):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(latent_size+label_size, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, n_channels*img_width*img_width),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, c):\n",
        "        x, c = x.view(x.size(0), -1), c.view(c.size(0), -1).float()\n",
        "        x = torch.cat((x, c), 1) # [input, label] concatenated\n",
        "        x = self.layer(x)\n",
        "        return x.view(x.size(0), n_channels, img_width, img_width)\n",
        "\n",
        "# define the discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, label_size=100):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(n_channels*img_width*img_width+label_size, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128,64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Sigmoid(),   \n",
        "        )\n",
        "    \n",
        "    def forward(self, x, c):        \n",
        "        x, c = x.view(x.size(0), -1), c.view(c.size(0), -1).float()\n",
        "        x = torch.cat((x, c), 1) # [input, label] concatenated\n",
        "        return self.layer(x)\n",
        "        \n",
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "\n",
        "print(f'Generator has {len(torch.nn.utils.parameters_to_vector(G.parameters()))} parameters.')\n",
        "print(f'Discriminator has {len(torch.nn.utils.parameters_to_vector(D.parameters()))} parameters')\n",
        "\n",
        "# initialise the optimiser\n",
        "optimiser_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimiser_D = torch.optim.Adam(D.parameters(), lr=0.0004, betas=(0.5, 0.999))\n",
        "print('Optimisers have been created!')\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "print('Loss function is Binary Cross Entropy!')\n",
        "\n",
        "epoch = 0\n",
        "# training loop\n",
        "while (epoch<50000):\n",
        "    \n",
        "    # arrays for metrics\n",
        "    logs = {}\n",
        "    gen_loss_arr = np.zeros(0)\n",
        "    dis_loss_arr = np.zeros(0)\n",
        "\n",
        "    # iterate over the train dateset\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "        x, t = batch\n",
        "        x, t = x.to(device), t.to(device)\n",
        "\n",
        "        # convert target labels \"t\" to a one-hot vector, e.g. 3 becomes [0,0,0,1,0,0,0,...]\n",
        "        y = torch.zeros(x.size(0), 100).long().to(device).scatter(1, t.view(x.size(0),1), 1)\n",
        "\n",
        "        # train discriminator\n",
        "        z = torch.randn(x.size(0), 100).to(device)\n",
        "        l_r = criterion(D(x,y),       torch.ones([64,1]).to(device)) # real -> 1\n",
        "        l_f = criterion(D(G(z,y),y), torch.zeros([64,1]).to(device)) # fake -> 0\n",
        "        loss_d = (l_r + l_f)/2.0\n",
        "        optimiser_D.zero_grad()\n",
        "        loss_d.backward()\n",
        "        optimiser_D.step()\n",
        "        \n",
        "        # train generator\n",
        "        z = torch.randn(x.size(0), 100).to(device)\n",
        "        loss_g = criterion(D(G(z,y),y), torch.ones([64,1]).to(device)) # fake -> 1\n",
        "        optimiser_G.zero_grad()\n",
        "        loss_g.backward()\n",
        "        optimiser_G.step()\n",
        "\n",
        "        gen_loss_arr = np.append(gen_loss_arr, loss_g.item())\n",
        "        dis_loss_arr = np.append(dis_loss_arr, loss_d.item())\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/20000], Discriminator Loss: {dis_loss_arr.mean():.4f}, Generator Loss: {gen_loss_arr.mean():.4f}')\n",
        "\n",
        "    # epoch = epoch+1\n",
        "    if (epoch + 1) % check_interval == 0:\n",
        "        G.eval()\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(batch_size, 100).to(device)\n",
        "            fake_labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
        "            y_fake = torch.zeros(batch_size, 100).to(device).scatter(1, fake_labels.view(-1,1), 1)\n",
        "            # z = z.unsqueeze(2).unsqueeze(3)\n",
        "            fake_images = G(z, y_fake)\n",
        "            fake_images = (fake_images + 1) / 2\n",
        "            print(f'Epoch [{epoch+1}/{num_of_epochs}], Loss D: {dis_loss_arr.mean():.4f}, Loss G: {gen_loss_arr.mean():.4f}')\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            plt.axis('off')\n",
        "            plt.title(f'Generated Images at Epoch {epoch+1}')\n",
        "            plt.imshow(np.transpose(torchvision.utils.make_grid(fake_images[:batch_size], padding=2, normalize=True).cpu(), (1, 2, 0)))\n",
        "            plt.show()\n",
        "        G.train()\n",
        "    \n",
        "    epoch = epoch + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew-Ik2p6pIkW"
      },
      "source": [
        "**Latent interpolations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "Sbb3a--HkGzZ",
        "outputId": "f8d8cbcd-f052-48a7-997e-9b44f42695ab"
      },
      "outputs": [],
      "source": [
        "# now show some interpolations (note you do not have to do linear interpolations as shown here, you can do non-linear or gradient-based interpolation if you wish)\n",
        "col_size = int(np.sqrt(batch_size))\n",
        "\n",
        "z0 = z[0:col_size].repeat(col_size,1) # z for top row\n",
        "z1 = z[batch_size-col_size:].repeat(col_size,1) # z for bottom row\n",
        "\n",
        "t = torch.linspace(0,1,col_size).unsqueeze(1).repeat(1,col_size).view(batch_size,1).to(device)\n",
        "\n",
        "lerp_z = (1-t)*z0 + t*z1 # linearly interpolate between two points in the latent space\n",
        "lerp_g = generator.sample(lerp_z) # sample the model at the resulting interpolated latents\n",
        "\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.grid(False)\n",
        "plt.imshow(torchvision.utils.make_grid(lerp_g).cpu().numpy().transpose(1, 2, 0), cmap=plt.cm.binary)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Vxe_qIbRXS"
      },
      "source": [
        "**FID scores**\n",
        "\n",
        "Evaluate the FID from 10k of your model samples (do not sample more than this) and compare it against the 10k test images. Calculating FID is somewhat involved, so we use a library for it. It can take a few minutes to evaluate. Lower FID scores are better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4RmRO6U7mLbq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install clean-fid\n",
        "import os\n",
        "from cleanfid import fid\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "canqHlS7bRXT"
      },
      "outputs": [],
      "source": [
        "# define directories\n",
        "real_images_dir = 'real_images'\n",
        "generated_images_dir = 'generated_images'\n",
        "num_samples = 10000 # do not change\n",
        "\n",
        "# create/clean the directories\n",
        "def setup_directory(directory):\n",
        "    if os.path.exists(directory):\n",
        "        !rm -r {directory} # remove any existing (old) data\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# setup_directory(real_images_dir)\n",
        "# setup_directory(generated_images_dir)\n",
        "\n",
        "# generate and save 10k model samples\n",
        "num_generated = 0\n",
        "while num_generated < num_samples:\n",
        "\n",
        "    # sample from your model, you can modify this\n",
        "    z = torch.randn(batch_size, latent_dim).to(device)\n",
        "    samples_batch = N.sample(z).cpu().detach()\n",
        "\n",
        "    for image in samples_batch:\n",
        "        if num_generated >= num_samples:\n",
        "            break\n",
        "        save_image(image, os.path.join(generated_images_dir, f\"gen_img_{num_generated}.png\"))\n",
        "        num_generated += 1\n",
        "\n",
        "# save 10k images from the CIFAR-100 test dataset\n",
        "num_saved_real = 0\n",
        "while num_saved_real < num_samples:\n",
        "    real_samples_batch, _ = next(test_iterator)\n",
        "    for image in real_samples_batch:\n",
        "        if num_saved_real >= num_samples:\n",
        "            break\n",
        "        save_image(image, os.path.join(real_images_dir, f\"real_img_{num_saved_real}.png\"))\n",
        "        num_saved_real += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RVOmtzabRXT",
        "outputId": "33d56f24-a300-4a81-e8f7-cfaedc180411"
      },
      "outputs": [],
      "source": [
        "# compute FID\n",
        "score = fid.compute_fid(real_images_dir, generated_images_dir, mode=\"clean\")\n",
        "print(f\"FID score: {score}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
