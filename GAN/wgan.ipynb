{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as disp\n",
    "from torch.nn.utils import spectral_norm    \n",
    "from torch import optim;\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to make getting another batch of data easier\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "class_names = ['apple','aquarium_fish','baby','bear','beaver','bed','bee','beetle','bicycle','bottle','bowl','boy','bridge','bus','butterfly','camel','can','castle','caterpillar','cattle','chair','chimpanzee','clock','cloud','cockroach','couch','crab','crocodile','cup','dinosaur','dolphin','elephant','flatfish','forest','fox','girl','hamster','house','kangaroo','computer_keyboard','lamp','lawn_mower','leopard','lion','lizard','lobster','man','maple_tree','motorcycle','mountain','mouse','mushroom','oak_tree','orange','orchid','otter','palm_tree','pear','pickup_truck','pine_tree','plain','plate','poppy','porcupine','possum','rabbit','raccoon','ray','road','rocket','rose','sea','seal','shark','shrew','skunk','skyscraper','snail','snake','spider','squirrel','streetcar','sunflower','sweet_pepper','table','tank','telephone','television','tiger','tractor','train','trout','tulip','turtle','wardrobe','whale','willow_tree','wolf','woman','worm',]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR100('data', train=True, download=True, transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])),\n",
    "    batch_size=32, drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR100('data', train=False, download=True, transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize([32,32]),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])),\n",
    "    batch_size=32, drop_last=True)\n",
    "\n",
    "train_iterator = iter(cycle(train_loader))\n",
    "test_iterator = iter(cycle(test_loader))\n",
    "\n",
    "print(f'> Size of training dataset {len(train_loader.dataset)}')\n",
    "print(f'> Size of test dataset {len(test_loader.dataset)}')\n",
    "print(\"Number of classes: \", len(class_names))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "num_batches_per_epoch = len(train_loader.dataset) // batch_size\n",
    "\n",
    "num_of_epochs = 50000 // num_batches_per_epoch\n",
    "\n",
    "print(\"Number of batches per epoch: \", num_batches_per_epoch)\n",
    "print(\"Number of epochs: \", num_of_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "image_channels = 3\n",
    "feature_dim = 64\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "critic_iterations = 5\n",
    "lambda_gp = 10\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# Generator Network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Initial dense layer to reshape noise vector\n",
    "        self.fc = nn.Linear(latent_dim, 4 * 4 * feature_dim * 8)\n",
    "        \n",
    "        # Main convolutional architecture\n",
    "        self.main = nn.Sequential(\n",
    "            # State size: (feature_dim*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(feature_dim * 8, feature_dim * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_dim * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # State size: (feature_dim*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(feature_dim * 4, feature_dim * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_dim * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # State size: (feature_dim*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(feature_dim * 2, feature_dim, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_dim),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Final layer to generate images\n",
    "            # State size: feature_dim x 32 x 32\n",
    "            nn.ConvTranspose2d(feature_dim, image_channels, 1, 1, 0, bias=False),\n",
    "            nn.Tanh()\n",
    "            # Output size: 3 x 32 x 32 (CIFAR-100 size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, feature_dim * 8, 4, 4)\n",
    "        return self.main(x)\n",
    "\n",
    "# Critic Network (Discriminator)\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # Input size: 3 x 32 x 32\n",
    "            nn.Conv2d(image_channels, feature_dim, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # State size: feature_dim x 16 x 16\n",
    "            nn.Conv2d(feature_dim, feature_dim * 2, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(feature_dim * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # State size: (feature_dim*2) x 8 x 8\n",
    "            nn.Conv2d(feature_dim * 2, feature_dim * 4, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(feature_dim * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # State size: (feature_dim*4) x 4 x 4\n",
    "            nn.Conv2d(feature_dim * 4, feature_dim * 8, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(feature_dim * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # State size: (feature_dim*8) x 2 x 2\n",
    "            nn.Conv2d(feature_dim * 8, 1, 2, 1, 0, bias=False)\n",
    "            # Output: 1 x 1 x 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(-1)\n",
    "\n",
    "# Gradient Penalty\n",
    "def compute_gradient_penalty(critic, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.rand((real_samples.size(0), 1, 1, 1)).to(device)\n",
    "    \n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    \n",
    "    # Calculate critic scores\n",
    "    d_interpolates = critic(interpolates)\n",
    "    \n",
    "    # Get gradient w.r.t. interpolates\n",
    "    fake = torch.ones(real_samples.size(0)).to(device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def save_generated_images(generator, critic, epoch, device, save_dir='generated_images', num_images=16):\n",
    "    \"\"\"Save a grid of generated images with generator outputs and critic scores\"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate images\n",
    "    z = torch.randn(num_images, latent_dim).to(device)\n",
    "    generator.eval()\n",
    "    critic.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate fake images\n",
    "        fake_imgs = generator(z)\n",
    "        # Get critic scores\n",
    "        critic_scores = critic(fake_imgs)\n",
    "        \n",
    "        # Move to CPU and denormalize\n",
    "        fake_imgs = fake_imgs.cpu()\n",
    "        fake_imgs = fake_imgs * 0.5 + 0.5  # Denormalize\n",
    "        critic_scores = critic_scores.cpu().numpy()\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, axs = plt.subplots(4, 4, figsize=(15, 15))\n",
    "    fig.suptitle(f'Generated Images (Epoch {epoch})', fontsize=16)\n",
    "    \n",
    "    # Convert images to grid\n",
    "    grid = make_grid(fake_imgs, nrow=4, normalize=False)\n",
    "    grid = grid.permute(1, 2, 0)  # Convert from CxHxW to HxWxC\n",
    "    \n",
    "    # Plot each image with its critic score\n",
    "    for idx, ax in enumerate(axs.flat):\n",
    "        if idx < num_images:\n",
    "            # Extract individual image from grid\n",
    "            img_size = grid.shape[0] // 4\n",
    "            row = idx // 4\n",
    "            col = idx % 4\n",
    "            img = grid[row*img_size:(row+1)*img_size, \n",
    "                      col*img_size:(col+1)*img_size, :]\n",
    "            \n",
    "            # Display image\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Add critic score as title\n",
    "            critic_score = critic_scores[idx]\n",
    "            ax.set_title(f'{class_names[idx]}\\nCritic Score:{critic_score:.3f}', fontsize=10)\n",
    "\n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'fake_images_epoch_{epoch}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    generator.train()\n",
    "    critic.train()\n",
    "\n",
    "def train():\n",
    "    total_iterations = num_epochs * num_batches_per_epoch\n",
    "    \n",
    "    for iteration in tqdm(range(total_iterations)):\n",
    "        # Calculate current epoch for logging\n",
    "        current_epoch = iteration // num_batches_per_epoch\n",
    "        \n",
    "        # Get next batch using our infinite iterator\n",
    "        real_imgs, _ = next(train_iterator)\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        batch_size = real_imgs.size(0)\n",
    "            \n",
    "        # Train Critic\n",
    "        for _ in range(critic_iterations):\n",
    "            c_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate fake images\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_imgs = generator(z)\n",
    "            \n",
    "            # Critic loss\n",
    "            real_validity = critic(real_imgs)\n",
    "            fake_validity = critic(fake_imgs.detach())\n",
    "            \n",
    "            # Gradient penalty\n",
    "            gp = compute_gradient_penalty(critic, real_imgs.data, fake_imgs.data)\n",
    "            \n",
    "            # Wasserstein distance\n",
    "            c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp\n",
    "            \n",
    "            c_loss.backward()\n",
    "            c_optimizer.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        # Generate fake images\n",
    "        z = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_imgs = generator(z)\n",
    "        \n",
    "        # Generator loss\n",
    "        fake_validity = critic(fake_imgs)\n",
    "        g_loss = -torch.mean(fake_validity)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        # Print progress\n",
    "        if iteration % 100 == 0:\n",
    "            print(\n",
    "                f\"[Epoch {current_epoch}/{num_epochs}] \"\n",
    "                f\"[C loss: {c_loss.item():.4f}] \"\n",
    "                f\"[G loss: {g_loss.item():.4f}]\"\n",
    "            )\n",
    "        \n",
    "        # Save generated images every 5 epochs\n",
    "        if current_epoch > 0 and current_epoch % 5 == 0 and iteration % num_batches_per_epoch == 0:\n",
    "            save_generated_images(generator, critic, current_epoch, device)\n",
    "\n",
    "# Initialize networks and optimizers\n",
    "generator = Generator().to(device)\n",
    "\n",
    "print(\"Parameters: \", sum(p.numel() for p in generator.parameters() if p.requires_grad))\n",
    "\n",
    "critic = Critic().to(device)\n",
    "\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "c_optimizer = optim.Adam(critic.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
