{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N22Uz-kLiZW"
      },
      "source": [
        "**Main imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK1Jl7nkLnPA"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as disp\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device.type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run1dh_hM0oO"
      },
      "source": [
        "**Import dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK383zeDM4Ac",
        "outputId": "8a35453f-df4e-4cb6-c791-f88d713097c4"
      },
      "outputs": [],
      "source": [
        "# helper function to make getting another batch of data easier\n",
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for x in iterable:\n",
        "            yield x\n",
        "\n",
        "class_names = ['apple','aquarium_fish','baby','bear','beaver','bed','bee','beetle','bicycle','bottle','bowl','boy','bridge','bus','butterfly','camel','can','castle','caterpillar','cattle','chair','chimpanzee','clock','cloud','cockroach','couch','crab','crocodile','cup','dinosaur','dolphin','elephant','flatfish','forest','fox','girl','hamster','house','kangaroo','computer_keyboard','lamp','lawn_mower','leopard','lion','lizard','lobster','man','maple_tree','motorcycle','mountain','mouse','mushroom','oak_tree','orange','orchid','otter','palm_tree','pear','pickup_truck','pine_tree','plain','plate','poppy','porcupine','possum','rabbit','raccoon','ray','road','rocket','rose','sea','seal','shark','shrew','skunk','skyscraper','snail','snake','spider','squirrel','streetcar','sunflower','sweet_pepper','table','tank','telephone','television','tiger','tractor','train','trout','tulip','turtle','wardrobe','whale','willow_tree','wolf','woman','worm',]\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.CIFAR100('data', train=True, download=True, transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor()\n",
        "    ])),\n",
        "    batch_size=64, drop_last=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.CIFAR100('data', train=False, download=True, transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor()\n",
        "    ])),\n",
        "    batch_size=64, drop_last=True)\n",
        "\n",
        "train_iterator = iter(cycle(train_loader))\n",
        "test_iterator = iter(cycle(test_loader))\n",
        "\n",
        "print(f'> Size of training dataset {len(train_loader.dataset)}')\n",
        "print(f'> Size of test dataset {len(test_loader.dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-FdW5HnimG2"
      },
      "source": [
        "**View some of the test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "BtJs-qxHRLXz",
        "outputId": "90986b66-733e-41fc-d01f-ddc442c4423e"
      },
      "outputs": [],
      "source": [
        "# let's view some of the training data\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "x,t = next(train_iterator)\n",
        "x,t = x.to(device), t.to(device)\n",
        "plt.imshow(torchvision.utils.make_grid(x).cpu().numpy().transpose(1, 2, 0), cmap=plt.cm.binary)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnjh12UbNFpV"
      },
      "source": [
        "**This is an autoencoder pretending to be a generative model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGbLY6X-NH4O",
        "outputId": "7ab14a92-ef22-4b6e-b8ea-87898019d9d1"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Linear(32*32*params['n_channels'], params['n_latent'])\n",
        "        self.decoder = nn.Linear(params['n_latent'], 32*32*params['n_channels'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x.view(x.size(0), -1))\n",
        "        z += 0.1*torch.randn_like(z) # crude attempt to make latent space normally distributed so we can sample it\n",
        "        x = torch.sigmoid(self.decoder(z))\n",
        "        return x.view(x.size(0), params['n_channels'], 32, 32)\n",
        "\n",
        "    def sample(self, z): # sample from some prior distribution (it should not depend on x)\n",
        "        x = torch.sigmoid(self.decoder(z))\n",
        "        return x.view(x.size(0), params['n_channels'], 32, 32)\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "params = {\n",
        "    'batch_size': train_loader.batch_size,\n",
        "    'n_channels': 3,\n",
        "    'n_latent': 7 # alters number of parameters\n",
        "}\n",
        "\n",
        "N = Autoencoder(params).to(device)\n",
        "\n",
        "print(f'> Number of model parameters {len(torch.nn.utils.parameters_to_vector(N.parameters()))}')\n",
        "if len(torch.nn.utils.parameters_to_vector(N.parameters())) > 1000000:\n",
        "    print(\"> Warning: you have gone over your parameter budget and will have a grade penalty!\")\n",
        "\n",
        "# initialise the optimiser\n",
        "optimiser = torch.optim.Adam(N.parameters(), lr=0.001)\n",
        "steps = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1UBl0PJjY-f"
      },
      "source": [
        "**Main training loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "uhk3MscpbRXR",
        "outputId": "1b3d105c-7150-4a46-aeff-a7cc141e7ce9"
      },
      "outputs": [],
      "source": [
        "# keep within our optimisation step budget\n",
        "while (steps < 50000):\n",
        "\n",
        "    # arrays for metrics\n",
        "    loss_arr = np.zeros(0)\n",
        "\n",
        "    # iterate over some of the train dateset\n",
        "    for i in range(1000):\n",
        "        x,t = next(train_iterator)\n",
        "        x,t = x.to(device), t.to(device)\n",
        "\n",
        "        # train model\n",
        "        p = N(x)\n",
        "        loss = F.mse_loss(p, x)\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "        steps += 1\n",
        "\n",
        "        loss_arr = np.append(loss_arr, loss.item())\n",
        "\n",
        "    print('steps {:.2f}, loss: {:.3f}'.format(steps, loss_arr.mean()))\n",
        "\n",
        "    # sample model and visualise results (ensure your sampling code does not use x)\n",
        "    N.eval()\n",
        "    z = torch.randn(params['batch_size'], params['n_latent']).to(device)\n",
        "    samples = N.sample(z).cpu().detach()\n",
        "    plt.imshow(torchvision.utils.make_grid(samples).cpu().numpy().transpose(1, 2, 0), cmap=plt.cm.binary)\n",
        "    plt.show()\n",
        "    disp.clear_output(wait=True)\n",
        "    N.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew-Ik2p6pIkW"
      },
      "source": [
        "**Latent interpolations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "Sbb3a--HkGzZ",
        "outputId": "f8d8cbcd-f052-48a7-997e-9b44f42695ab"
      },
      "outputs": [],
      "source": [
        "# now show some interpolations (note you do not have to do linear interpolations as shown here, you can do non-linear or gradient-based interpolation if you wish)\n",
        "col_size = int(np.sqrt(params['batch_size']))\n",
        "\n",
        "z0 = z[0:col_size].repeat(col_size,1) # z for top row\n",
        "z1 = z[params['batch_size']-col_size:].repeat(col_size,1) # z for bottom row\n",
        "\n",
        "t = torch.linspace(0,1,col_size).unsqueeze(1).repeat(1,col_size).view(params['batch_size'],1).to(device)\n",
        "\n",
        "lerp_z = (1-t)*z0 + t*z1 # linearly interpolate between two points in the latent space\n",
        "lerp_g = N.sample(lerp_z) # sample the model at the resulting interpolated latents\n",
        "\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.grid(False)\n",
        "plt.imshow(torchvision.utils.make_grid(lerp_g).cpu().numpy().transpose(1, 2, 0), cmap=plt.cm.binary)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Vxe_qIbRXS"
      },
      "source": [
        "**FID scores**\n",
        "\n",
        "Evaluate the FID from 10k of your model samples (do not sample more than this) and compare it against the 10k test images. Calculating FID is somewhat involved, so we use a library for it. It can take a few minutes to evaluate. Lower FID scores are better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4RmRO6U7mLbq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install clean-fid\n",
        "import os\n",
        "from cleanfid import fid\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "canqHlS7bRXT"
      },
      "outputs": [],
      "source": [
        "# define directories\n",
        "real_images_dir = 'real_images'\n",
        "generated_images_dir = 'generated_images'\n",
        "num_samples = 10000 # do not change\n",
        "\n",
        "# create/clean the directories\n",
        "def setup_directory(directory):\n",
        "    if os.path.exists(directory):\n",
        "        !rm -r {directory} # remove any existing (old) data\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# setup_directory(real_images_dir)\n",
        "# setup_directory(generated_images_dir)\n",
        "\n",
        "# generate and save 10k model samples\n",
        "num_generated = 0\n",
        "while num_generated < num_samples:\n",
        "\n",
        "    # sample from your model, you can modify this\n",
        "    z = torch.randn(params['batch_size'], params['n_latent']).to(device)\n",
        "    samples_batch = N.sample(z).cpu().detach()\n",
        "\n",
        "    for image in samples_batch:\n",
        "        if num_generated >= num_samples:\n",
        "            break\n",
        "        save_image(image, os.path.join(generated_images_dir, f\"gen_img_{num_generated}.png\"))\n",
        "        num_generated += 1\n",
        "\n",
        "# save 10k images from the CIFAR-100 test dataset\n",
        "num_saved_real = 0\n",
        "while num_saved_real < num_samples:\n",
        "    real_samples_batch, _ = next(test_iterator)\n",
        "    for image in real_samples_batch:\n",
        "        if num_saved_real >= num_samples:\n",
        "            break\n",
        "        save_image(image, os.path.join(real_images_dir, f\"real_img_{num_saved_real}.png\"))\n",
        "        num_saved_real += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RVOmtzabRXT",
        "outputId": "33d56f24-a300-4a81-e8f7-cfaedc180411"
      },
      "outputs": [],
      "source": [
        "# compute FID\n",
        "score = fid.compute_fid(real_images_dir, generated_images_dir, mode=\"clean\")\n",
        "print(f\"FID score: {score}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
