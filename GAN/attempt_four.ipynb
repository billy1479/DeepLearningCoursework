{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N22Uz-kLiZW"
      },
      "source": [
        "**Main imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK1Jl7nkLnPA"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as disp\n",
        "from torch.nn.utils import spectral_norm    \n",
        "import os\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device.type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run1dh_hM0oO"
      },
      "source": [
        "**Import dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK383zeDM4Ac",
        "outputId": "8a35453f-df4e-4cb6-c791-f88d713097c4"
      },
      "outputs": [],
      "source": [
        "# helper function to make getting another batch of data easier\n",
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for x in iterable:\n",
        "            yield x\n",
        "\n",
        "class_names = ['apple','aquarium_fish','baby','bear','beaver','bed','bee','beetle','bicycle','bottle','bowl','boy','bridge','bus','butterfly','camel','can','castle','caterpillar','cattle','chair','chimpanzee','clock','cloud','cockroach','couch','crab','crocodile','cup','dinosaur','dolphin','elephant','flatfish','forest','fox','girl','hamster','house','kangaroo','computer_keyboard','lamp','lawn_mower','leopard','lion','lizard','lobster','man','maple_tree','motorcycle','mountain','mouse','mushroom','oak_tree','orange','orchid','otter','palm_tree','pear','pickup_truck','pine_tree','plain','plate','poppy','porcupine','possum','rabbit','raccoon','ray','road','rocket','rose','sea','seal','shark','shrew','skunk','skyscraper','snail','snake','spider','squirrel','streetcar','sunflower','sweet_pepper','table','tank','telephone','television','tiger','tractor','train','trout','tulip','turtle','wardrobe','whale','willow_tree','wolf','woman','worm',]\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.CIFAR100('data', train=True, download=True, transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])),\n",
        "    batch_size=64, drop_last=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.CIFAR100('data', train=False, download=True, transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize([32,32]),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])),\n",
        "    batch_size=64, drop_last=True)\n",
        "\n",
        "train_iterator = iter(cycle(train_loader))\n",
        "test_iterator = iter(cycle(test_loader))\n",
        "\n",
        "print(f'> Size of training dataset {len(train_loader.dataset)}')\n",
        "print(f'> Size of test dataset {len(test_loader.dataset)}')\n",
        "print(\"Number of classes: \", len(class_names))\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "num_batches_per_epoch = len(train_loader.dataset) // batch_size\n",
        "\n",
        "num_of_epochs = 50000 // num_batches_per_epoch\n",
        "\n",
        "print(\"Number of batches per epoch: \", num_batches_per_epoch)\n",
        "print(\"Number of epochs: \", num_of_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-FdW5HnimG2"
      },
      "source": [
        "**View some of the test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "BtJs-qxHRLXz",
        "outputId": "90986b66-733e-41fc-d01f-ddc442c4423e"
      },
      "outputs": [],
      "source": [
        "# let's view some of the training data\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "x, t = next(train_iterator)\n",
        "\n",
        "# Ensure the tensor is correctly moved to the GPU\n",
        "x = x.to(device)\n",
        "t = t.to(device)\n",
        "\n",
        "# Plot the images\n",
        "plt.imshow(torchvision.utils.make_grid(x).cpu().numpy().transpose(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generator, Discriminator, and Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = train_loader.batch_size\n",
        "# channels = 3\n",
        "# latent_dim = 256\n",
        "# image_size = 32\n",
        "\n",
        "num_classes = 100\n",
        "\n",
        "check_interval = 5\n",
        "\n",
        "num_of_epochs = 50000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Repo: https://github.com/atapour/dl-pytorch/blob/main/Conditional_GAN_Example/Conditional_GAN_Example.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_size=100, label_size=100, channels=3):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        self.label_embedding = nn.Linear(label_size, latent_size)\n",
        "        \n",
        "        self.init_size = 4\n",
        "        self.l1 = nn.Sequential(\n",
        "            nn.Linear(latent_size * 2, 128 * self.init_size ** 2)\n",
        "        )\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        label_embedding = self.label_embedding(labels)\n",
        "        x = torch.cat((noise, label_embedding), dim=1)\n",
        "        x = self.l1(x)\n",
        "        x = x.view(x.shape[0], 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(x)\n",
        "        return img\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, label_size=100, channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.label_embedding = nn.Linear(label_size, 32 * 32)\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            # input is (channels) x 32 x 32\n",
        "            nn.Conv2d(channels + 1, 64, 3, 2, 1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout2d(0.25),\n",
        "            # state size: 64 x 16 x 16\n",
        "            nn.Conv2d(64, 128, 3, 2, 1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout2d(0.25),\n",
        "            nn.BatchNorm2d(128),\n",
        "            # state size: 128 x 8 x 8\n",
        "            nn.Conv2d(128, 256, 3, 2, 1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout2d(0.25),\n",
        "            nn.BatchNorm2d(256),\n",
        "            # state size: 256 x 4 x 4\n",
        "        )\n",
        "\n",
        "        self.adv_layer = nn.Sequential(\n",
        "            nn.Linear(256 * 4 * 4, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img, labels):\n",
        "        label_embedding = self.label_embedding(labels)\n",
        "        label_embedding = label_embedding.view(-1, 1, 32, 32)\n",
        "        d_in = torch.cat((img, label_embedding), 1)\n",
        "        \n",
        "        features = self.conv_blocks(d_in)\n",
        "        features = features.view(features.shape[0], -1)\n",
        "        validity = self.adv_layer(features)\n",
        "        return validity\n",
        "\n",
        "# Improved training parameters\n",
        "batch_size = 128\n",
        "lr_g = 1e-4\n",
        "lr_d = 4e-4\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "\n",
        "# Initialize networks and optimizers\n",
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "\n",
        "# Use different learning rates for G and D\n",
        "optimizerG = torch.optim.Adam(G.parameters(), lr=lr_g, betas=(beta1, beta2))\n",
        "optimizerD = torch.optim.Adam(D.parameters(), lr=lr_d, betas=(beta1, beta2))\n",
        "\n",
        "# Use label smoothing for real labels\n",
        "def get_smooth_real_labels(size):\n",
        "    return torch.FloatTensor(size, 1).uniform_(0.9, 1.0).to(device)\n",
        "\n",
        "# Training modifications\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "def compute_gradient_penalty(D, real_samples, fake_samples, labels):\n",
        "    alpha = torch.rand(real_samples.size(0), 1, 1, 1).to(device)\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "    d_interpolates = D(interpolates, labels)\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones_like(d_interpolates),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew-Ik2p6pIkW"
      },
      "source": [
        "**Latent interpolations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "Sbb3a--HkGzZ",
        "outputId": "f8d8cbcd-f052-48a7-997e-9b44f42695ab"
      },
      "outputs": [],
      "source": [
        "# now show some interpolations (note you do not have to do linear interpolations as shown here, you can do non-linear or gradient-based interpolation if you wish)\n",
        "col_size = int(np.sqrt(batch_size))\n",
        "\n",
        "z0 = z[0:col_size].repeat(col_size,1) # z for top row\n",
        "z1 = z[batch_size-col_size:].repeat(col_size,1) # z for bottom row\n",
        "\n",
        "t = torch.linspace(0,1,col_size).unsqueeze(1).repeat(1,col_size).view(batch_size,1).to(device)\n",
        "\n",
        "lerp_z = (1-t)*z0 + t*z1 # linearly interpolate between two points in the latent space\n",
        "lerp_g = Generator.sample(lerp_z) # sample the model at the resulting interpolated latents\n",
        "\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.grid(False)\n",
        "plt.imshow(torchvision.utils.make_grid(lerp_g).cpu().numpy().transpose(1, 2, 0), cmap=plt.cm.binary)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Vxe_qIbRXS"
      },
      "source": [
        "**FID scores**\n",
        "\n",
        "Evaluate the FID from 10k of your model samples (do not sample more than this) and compare it against the 10k test images. Calculating FID is somewhat involved, so we use a library for it. It can take a few minutes to evaluate. Lower FID scores are better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4RmRO6U7mLbq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install clean-fid\n",
        "import os\n",
        "from cleanfid import fid\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "canqHlS7bRXT"
      },
      "outputs": [],
      "source": [
        "# define directories\n",
        "real_images_dir = 'real_images'\n",
        "generated_images_dir = 'generated_images'\n",
        "num_samples = 10000 # do not change\n",
        "\n",
        "# create/clean the directories\n",
        "def setup_directory(directory):\n",
        "    if os.path.exists(directory):\n",
        "        !rm -r {directory} # remove any existing (old) data\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# setup_directory(real_images_dir)\n",
        "# setup_directory(generated_images_dir)\n",
        "\n",
        "# generate and save 10k model samples\n",
        "num_generated = 0\n",
        "while num_generated < num_samples:\n",
        "\n",
        "    # sample from your model, you can modify this\n",
        "    z = torch.randn(batch_size, latent_dim).to(device)\n",
        "    samples_batch = N.sample(z).cpu().detach()\n",
        "\n",
        "    for image in samples_batch:\n",
        "        if num_generated >= num_samples:\n",
        "            break\n",
        "        save_image(image, os.path.join(generated_images_dir, f\"gen_img_{num_generated}.png\"))\n",
        "        num_generated += 1\n",
        "\n",
        "# save 10k images from the CIFAR-100 test dataset\n",
        "num_saved_real = 0\n",
        "while num_saved_real < num_samples:\n",
        "    real_samples_batch, _ = next(test_iterator)\n",
        "    for image in real_samples_batch:\n",
        "        if num_saved_real >= num_samples:\n",
        "            break\n",
        "        save_image(image, os.path.join(real_images_dir, f\"real_img_{num_saved_real}.png\"))\n",
        "        num_saved_real += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RVOmtzabRXT",
        "outputId": "33d56f24-a300-4a81-e8f7-cfaedc180411"
      },
      "outputs": [],
      "source": [
        "# compute FID\n",
        "score = fid.compute_fid(real_images_dir, generated_images_dir, mode=\"clean\")\n",
        "print(f\"FID score: {score}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
