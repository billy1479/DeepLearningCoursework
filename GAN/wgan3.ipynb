{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as disp\n",
    "from torch.nn.utils import spectral_norm    \n",
    "from torch import optim;\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device.type)\n",
    "\n",
    "# helper function to make getting another batch of data easier\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "class_names = ['apple','aquarium_fish','baby','bear','beaver','bed','bee','beetle','bicycle','bottle','bowl','boy','bridge','bus','butterfly','camel','can','castle','caterpillar','cattle','chair','chimpanzee','clock','cloud','cockroach','couch','crab','crocodile','cup','dinosaur','dolphin','elephant','flatfish','forest','fox','girl','hamster','house','kangaroo','computer_keyboard','lamp','lawn_mower','leopard','lion','lizard','lobster','man','maple_tree','motorcycle','mountain','mouse','mushroom','oak_tree','orange','orchid','otter','palm_tree','pear','pickup_truck','pine_tree','plain','plate','poppy','porcupine','possum','rabbit','raccoon','ray','road','rocket','rose','sea','seal','shark','shrew','skunk','skyscraper','snail','snake','spider','squirrel','streetcar','sunflower','sweet_pepper','table','tank','telephone','television','tiger','tractor','train','trout','tulip','turtle','wardrobe','whale','willow_tree','wolf','woman','worm',]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR100('data', train=True, download=True, transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])),\n",
    "    batch_size=64, drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR100('data', train=False, download=True, transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize([32,32]),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])),\n",
    "    batch_size=64, drop_last=True)\n",
    "\n",
    "train_iterator = iter(cycle(train_loader))\n",
    "test_iterator = iter(cycle(test_loader))\n",
    "\n",
    "print(f'> Size of training dataset {len(train_loader.dataset)}')\n",
    "print(f'> Size of test dataset {len(test_loader.dataset)}')\n",
    "print(\"Number of classes: \", len(class_names))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "num_batches_per_epoch = len(train_loader.dataset) // batch_size\n",
    "\n",
    "print(\"Length of train_loader: \", len(train_loader.dataset))\n",
    "\n",
    "num_of_epochs = 50000 // num_batches_per_epoch\n",
    "\n",
    "print(\"Number of batches per epoch: \", num_batches_per_epoch)\n",
    "print(\"Number of epochs: \", num_of_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Increased latent dimension and initial feature maps for better expressivity\n",
    "        self.init_size = 4\n",
    "        self.latent_dim = latent_dim\n",
    "        self.init_channels = 512  # Increased from 64\n",
    "        \n",
    "        # Larger dense projection with batch norm for stable training\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(latent_dim, self.init_channels * self.init_size ** 2),\n",
    "            nn.BatchNorm1d(self.init_channels * self.init_size ** 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Enhanced convolutional decoder with residual connections\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # 4x4x512 -> 8x8x256\n",
    "            nn.BatchNorm2d(self.init_channels),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(self.init_channels, self.init_channels//2, 3, 1, 1),\n",
    "            ResBlock(self.init_channels//2),\n",
    "            \n",
    "            # 8x8x256 -> 16x16x128\n",
    "            nn.BatchNorm2d(self.init_channels//2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(self.init_channels//2, self.init_channels//4, 3, 1, 1),\n",
    "            ResBlock(self.init_channels//4),\n",
    "            \n",
    "            # 16x16x128 -> 32x32x64\n",
    "            nn.BatchNorm2d(self.init_channels//4),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(self.init_channels//4, self.init_channels//8, 3, 1, 1),\n",
    "            ResBlock(self.init_channels//8),\n",
    "            \n",
    "            # Final refinement layers\n",
    "            nn.BatchNorm2d(self.init_channels//8),\n",
    "            nn.Conv2d(self.init_channels//8, self.init_channels//16, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(self.init_channels//16, 3, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Initialize weights for better training dynamics\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "                \n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], self.init_channels, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "        self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.activation(x + self.block(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        # Increased initial channels for better feature extraction\n",
    "        init_channels = 64  # Starting with more channels\n",
    "        \n",
    "        # Enhanced feature extraction with spectral normalization\n",
    "        self.features = nn.Sequential(\n",
    "            # 32x32x3 -> 16x16x64\n",
    "            spectral_norm(nn.Conv2d(3, init_channels, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            ResBlock(init_channels),\n",
    "            \n",
    "            # 16x16x64 -> 8x8x128\n",
    "            spectral_norm(nn.Conv2d(init_channels, init_channels*2, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            ResBlock(init_channels*2),\n",
    "            \n",
    "            # 8x8x128 -> 4x4x256\n",
    "            spectral_norm(nn.Conv2d(init_channels*2, init_channels*4, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            ResBlock(init_channels*4),\n",
    "            \n",
    "            # 4x4x256 -> 4x4x512\n",
    "            spectral_norm(nn.Conv2d(init_channels*4, init_channels*8, 3, 1, 1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # More expressive critic head\n",
    "        self.critic_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            spectral_norm(nn.Linear(init_channels*8 * 4 * 4, 1024)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            spectral_norm(nn.Linear(1024, 1))\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "                \n",
    "    def forward(self, img):\n",
    "        features = self.features(img)\n",
    "        validity = self.critic_head(features)\n",
    "        return validity\n",
    "\n",
    "class WGAN:\n",
    "    def __init__(self, latent_dim=64, clip_value=0.01, n_critic=5, device='cuda', class_names=None):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.clip_value = clip_value\n",
    "        self.n_critic = n_critic\n",
    "        self.device = device\n",
    "        self.class_names = class_names\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.generator = Generator(latent_dim).to(device)\n",
    "        self.critic = Critic().to(device)\n",
    "        \n",
    "        # Print model parameters\n",
    "        generator_params = sum(p.numel() for p in self.generator.parameters())\n",
    "        critic_params = sum(p.numel() for p in self.critic.parameters())\n",
    "        print(f\"Generator parameters: {generator_params:,}\")\n",
    "        print(f\"Critic parameters: {critic_params:,}\")\n",
    "        print(f\"Total parameters: {generator_params + critic_params:,}\")\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        # self.optimizer_G = optim.RMSprop(self.generator.parameters(), lr=0.0005)\n",
    "        # self.optimizer_C = optim.RMSprop(self.critic.parameters(), lr=0.0005)\n",
    "        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.optimizer_C = optim.Adam(self.critic.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        \n",
    "        self.steps = 0\n",
    "        self.G_losses = []\n",
    "        self.C_losses = []\n",
    "\n",
    "    def train_step(self, real_imgs):\n",
    "        batch_size = real_imgs.shape[0]\n",
    "        real_imgs = real_imgs.to(self.device)\n",
    "        \n",
    "        # Train Critic\n",
    "        self.optimizer_C.zero_grad()\n",
    "        z = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
    "        fake_imgs = self.generator(z).detach()\n",
    "        \n",
    "        real_validity = self.critic(real_imgs)\n",
    "        fake_validity = self.critic(fake_imgs)\n",
    "        critic_loss = -torch.mean(real_validity) + torch.mean(fake_validity)\n",
    "        \n",
    "        critic_loss.backward()\n",
    "        self.optimizer_C.step()\n",
    "        \n",
    "        for p in self.critic.parameters():\n",
    "            p.data.clamp_(-self.clip_value, self.clip_value)\n",
    "        \n",
    "        self.C_losses.append(critic_loss.item())\n",
    "        \n",
    "        # Train Generator\n",
    "        if self.steps % self.n_critic == 0:\n",
    "            self.optimizer_G.zero_grad()\n",
    "            gen_imgs = self.generator(z)\n",
    "            gen_validity = self.critic(gen_imgs)\n",
    "            generator_loss = -torch.mean(gen_validity)\n",
    "            \n",
    "            generator_loss.backward()\n",
    "            self.optimizer_G.step()\n",
    "            \n",
    "            self.G_losses.append(generator_loss.item())\n",
    "            return generator_loss.item(), critic_loss.item()\n",
    "        \n",
    "        self.steps += 1\n",
    "        return None, critic_loss.item()\n",
    "\n",
    "    def generate_samples(self, n_samples):\n",
    "        self.generator.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, self.latent_dim).to(self.device)\n",
    "            samples = self.generator(z)\n",
    "        self.generator.train()\n",
    "        return samples\n",
    "\n",
    "def train_wgan(num_batches_per_epoch, num_of_epochs, train_iterator, device='cuda', class_names=None):\n",
    "    import os\n",
    "    save_dir = 'generated_images'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    wgan = WGAN(latent_dim=128, clip_value=0.01, n_critic=5, device=device, class_names=class_names)\n",
    "    \n",
    "    total_steps = num_batches_per_epoch * num_of_epochs\n",
    "\n",
    "    print(\"Total number of steps: \", total_steps)\n",
    "    \n",
    "    for step in range(total_steps):\n",
    "        real_imgs, _ = next(train_iterator)\n",
    "        g_loss, c_loss = wgan.train_step(real_imgs)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            epoch = step // num_batches_per_epoch\n",
    "            batch = step % num_batches_per_epoch\n",
    "            print(f\"[Epoch {epoch}/{num_of_epochs}] \"\n",
    "                  f\"[Batch {batch}/{num_batches_per_epoch}] \"\n",
    "                  f\"[C loss: {c_loss:.4f}] \"\n",
    "                  + (f\"[G loss: {g_loss:.4f}]\" if g_loss is not None else \"\"))\n",
    "            \n",
    "            if step % 500 == 0:\n",
    "                samples = wgan.generate_samples(16)\n",
    "                save_path = os.path.join(save_dir, f'wgan_step_{step}.png')\n",
    "                torchvision.utils.save_image(samples,\n",
    "                                           save_path,\n",
    "                                           normalize=True,\n",
    "                                           nrow=4)\n",
    "                print(f\"Saved generated samples to {save_path}\")\n",
    "    \n",
    "    return wgan\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "wgan = train_wgan(num_batches_per_epoch, num_of_epochs, train_iterator, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
