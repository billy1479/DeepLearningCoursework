{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N22Uz-kLiZW"
      },
      "source": [
        "**Main imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK1Jl7nkLnPA"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as disp\n",
        "from torch.nn.utils import spectral_norm    \n",
        "from torch import optim;\n",
        "import os\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device.type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run1dh_hM0oO"
      },
      "source": [
        "**Import dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK383zeDM4Ac",
        "outputId": "8a35453f-df4e-4cb6-c791-f88d713097c4"
      },
      "outputs": [],
      "source": [
        "# helper function to make getting another batch of data easier\n",
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for x in iterable:\n",
        "            yield x\n",
        "\n",
        "class_names = ['apple','aquarium_fish','baby','bear','beaver','bed','bee','beetle','bicycle','bottle','bowl','boy','bridge','bus','butterfly','camel','can','castle','caterpillar','cattle','chair','chimpanzee','clock','cloud','cockroach','couch','crab','crocodile','cup','dinosaur','dolphin','elephant','flatfish','forest','fox','girl','hamster','house','kangaroo','computer_keyboard','lamp','lawn_mower','leopard','lion','lizard','lobster','man','maple_tree','motorcycle','mountain','mouse','mushroom','oak_tree','orange','orchid','otter','palm_tree','pear','pickup_truck','pine_tree','plain','plate','poppy','porcupine','possum','rabbit','raccoon','ray','road','rocket','rose','sea','seal','shark','shrew','skunk','skyscraper','snail','snake','spider','squirrel','streetcar','sunflower','sweet_pepper','table','tank','telephone','television','tiger','tractor','train','trout','tulip','turtle','wardrobe','whale','willow_tree','wolf','woman','worm',]\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.CIFAR100('data', train=True, download=True, transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])),\n",
        "    batch_size=64, drop_last=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.CIFAR100('data', train=False, download=True, transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize([32,32]),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])),\n",
        "    batch_size=64, drop_last=True)\n",
        "\n",
        "train_iterator = iter(cycle(train_loader))\n",
        "test_iterator = iter(cycle(test_loader))\n",
        "\n",
        "print(f'> Size of training dataset {len(train_loader.dataset)}')\n",
        "print(f'> Size of test dataset {len(test_loader.dataset)}')\n",
        "print(\"Number of classes: \", len(class_names))\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "num_batches_per_epoch = len(train_loader.dataset) // batch_size\n",
        "\n",
        "num_of_epochs = 50000 // num_batches_per_epoch\n",
        "\n",
        "print(\"Number of batches per epoch: \", num_batches_per_epoch)\n",
        "print(\"Number of epochs: \", num_of_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-FdW5HnimG2"
      },
      "source": [
        "**View some of the test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "BtJs-qxHRLXz",
        "outputId": "90986b66-733e-41fc-d01f-ddc442c4423e"
      },
      "outputs": [],
      "source": [
        "# let's view some of the training data\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "x, t = next(train_iterator)\n",
        "\n",
        "# Ensure the tensor is correctly moved to the GPU\n",
        "x = x.to(device)\n",
        "t = t.to(device)\n",
        "\n",
        "# Plot the images\n",
        "plt.imshow(torchvision.utils.make_grid(x).cpu().numpy().transpose(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generator, Discriminator, and Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = 100\n",
        "check_interval = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Repo: https://github.com/atapour/dl-pytorch/blob/main/Conditional_GAN_Example/Conditional_GAN_Example.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from scipy import linalg\n",
        "from torchvision.models.inception import inception_v3\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"Self-attention block for capturing global dependencies\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.query = nn.Conv2d(channels, channels // 8, 1)\n",
        "        self.key = nn.Conv2d(channels, channels // 8, 1)\n",
        "        self.value = nn.Conv2d(channels, channels, 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size, C, H, W = x.size()\n",
        "        \n",
        "        # Generate query, key, value projections\n",
        "        query = self.query(x).view(batch_size, -1, H * W).permute(0, 2, 1)\n",
        "        key = self.key(x).view(batch_size, -1, H * W)\n",
        "        value = self.value(x).view(batch_size, -1, H * W)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        attention = torch.bmm(query, key)\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, H, W)\n",
        "        \n",
        "        return self.gamma * out + x\n",
        "\n",
        "class EnhancedResBlock(nn.Module):\n",
        "    \"\"\"Enhanced residual block with grouped convolutions and squeeze-excitation\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, groups=8):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1, groups=groups)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, groups=groups)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        # Squeeze-Excitation block\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(out_channels, out_channels // 16, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels // 16, out_channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        # Skip connection with 1x1 conv if needed\n",
        "        self.skip = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        identity = self.skip(x)\n",
        "        \n",
        "        out = F.leaky_relu(self.bn1(self.conv1(x)), 0.2)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        \n",
        "        # Apply squeeze-excitation\n",
        "        se_weight = self.se(out)\n",
        "        out = out * se_weight\n",
        "        \n",
        "        out += identity\n",
        "        out = F.leaky_relu(out, 0.2)\n",
        "        return out\n",
        "\n",
        "class ImprovedVAE(nn.Module):\n",
        "    def __init__(self, latent_dim=256):  # Increased latent dimension\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        \n",
        "        # Encoder with enhanced blocks and attention\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            \n",
        "            EnhancedResBlock(64, 128),\n",
        "            AttentionBlock(128),\n",
        "            nn.AvgPool2d(2),  # 16x16\n",
        "            \n",
        "            EnhancedResBlock(128, 256),\n",
        "            AttentionBlock(256),\n",
        "            nn.AvgPool2d(2),  # 8x8\n",
        "            \n",
        "            EnhancedResBlock(256, 512),\n",
        "            AttentionBlock(512),\n",
        "            nn.AvgPool2d(2),  # 4x4\n",
        "        )\n",
        "        \n",
        "        self.flatten_dim = 4 * 4 * 512\n",
        "        \n",
        "        # Latent space projections with increased capacity\n",
        "        self.fc_mu = nn.Sequential(\n",
        "            nn.Linear(self.flatten_dim, self.flatten_dim // 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(self.flatten_dim // 2, latent_dim)\n",
        "        )\n",
        "        \n",
        "        self.fc_logvar = nn.Sequential(\n",
        "            nn.Linear(self.flatten_dim, self.flatten_dim // 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(self.flatten_dim // 2, latent_dim)\n",
        "        )\n",
        "        \n",
        "        # Decoder input\n",
        "        self.decoder_input = nn.Sequential(\n",
        "            nn.Linear(latent_dim, self.flatten_dim // 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(self.flatten_dim // 2, self.flatten_dim)\n",
        "        )\n",
        "        \n",
        "        # Decoder with enhanced blocks and attention\n",
        "        self.decoder = nn.Sequential(\n",
        "            EnhancedResBlock(512, 512),\n",
        "            AttentionBlock(512),\n",
        "            nn.Upsample(scale_factor=2),  # 8x8\n",
        "            \n",
        "            EnhancedResBlock(512, 256),\n",
        "            AttentionBlock(256),\n",
        "            nn.Upsample(scale_factor=2),  # 16x16\n",
        "            \n",
        "            EnhancedResBlock(256, 128),\n",
        "            AttentionBlock(128),\n",
        "            nn.Upsample(scale_factor=2),  # 32x32\n",
        "            \n",
        "            EnhancedResBlock(128, 64),\n",
        "            nn.Conv2d(64, 3, 3, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        \n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(-1, self.flatten_dim)\n",
        "        return self.fc_mu(x), self.fc_logvar(x)\n",
        "    \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "    \n",
        "    def decode(self, z):\n",
        "        x = self.decoder_input(z)\n",
        "        x = x.view(-1, 512, 4, 4)\n",
        "        return self.decoder(x)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "    \n",
        "    def generate(self, num_samples, device='cuda'):\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(num_samples, self.latent_dim).to(device)\n",
        "            return self.decode(z)\n",
        "\n",
        "def improved_vae_loss(recon_x, x, mu, logvar, kld_weight=0.0005):  # Reduced KLD weight\n",
        "    \"\"\"Enhanced VAE loss combining L1, MSE, and perceptual losses\"\"\"\n",
        "    # Reconstruction loss (combination of L1 and MSE)\n",
        "    mse_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
        "    l1_loss = F.l1_loss(recon_x, x, reduction='sum')\n",
        "    recon_loss = 0.5 * (mse_loss + l1_loss)\n",
        "    \n",
        "    # KL divergence loss with reduced weight\n",
        "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    \n",
        "    return recon_loss + kld_weight * kld_loss\n",
        "\n",
        "# Training setup with improved optimizer settings\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = ImprovedVAE().to(device)\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_of_epochs, eta_min=1e-6)\n",
        "\n",
        "def train_epoch(model, optimizer, train_iterator, device, kld_weight):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    images, _ = next(train_iterator)\n",
        "    images = images.to(device)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    recon_images, mu, logvar = model(images)\n",
        "    \n",
        "    loss = improved_vae_loss(recon_images, images, mu, logvar, kld_weight)\n",
        "    loss.backward()\n",
        "    \n",
        "    # Gradient clipping to prevent exploding gradients\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    \n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(model, test_iterator, device, num_batches=10):\n",
        "    \"\"\"Evaluate the model on test data\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_batches):\n",
        "            images, _ = next(test_iterator)\n",
        "            images = images.to(device)\n",
        "            \n",
        "            recon_images, mu, logvar = model(images)\n",
        "            loss = improved_vae_loss(recon_images, images, mu, logvar)\n",
        "            total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / num_batches\n",
        "def improved_vae_loss(recon_x, x, mu, logvar, kld_weight=0.0005):  # Reduced KLD weight\n",
        "    \"\"\"Enhanced VAE loss combining L1, MSE, and perceptual losses\"\"\"\n",
        "    # Reconstruction loss (combination of L1 and MSE)\n",
        "    mse_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
        "    l1_loss = F.l1_loss(recon_x, x, reduction='sum')\n",
        "    recon_loss = 0.5 * (mse_loss + l1_loss)\n",
        "    \n",
        "    # KL divergence loss with reduced weight\n",
        "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    \n",
        "    return recon_loss + kld_weight * kld_loss\n",
        "\n",
        "def visualize_results(model, images, epoch, device, directories):\n",
        "    \"\"\"\n",
        "    Visualize and save model outputs including reconstructions and generated samples\n",
        "    \n",
        "    Args:\n",
        "        model: The VAE model\n",
        "        images: Batch of original images\n",
        "        epoch: Current epoch number\n",
        "        device: Device to run the model on\n",
        "        directories: Dictionary containing output directory paths\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Generate reconstructions\n",
        "        recon_images, _, _ = model(images)\n",
        "        \n",
        "        # Create and save reconstruction comparison\n",
        "        comparison = torch.cat([images[:8], recon_images[:8]])\n",
        "        torchvision.utils.save_image(\n",
        "            comparison,\n",
        "            os.path.join(directories['reconstructions'], f'reconstruction_epoch_{epoch:04d}.png'),\n",
        "            normalize=True,\n",
        "            nrow=8\n",
        "        )\n",
        "        \n",
        "        # Generate and save new samples\n",
        "        samples = model.generate(16, device)\n",
        "        torchvision.utils.save_image(\n",
        "            samples,\n",
        "            os.path.join(directories['generated_images'], f'generated_epoch_{epoch:04d}.png'),\n",
        "            normalize=True,\n",
        "            nrow=4\n",
        "        )\n",
        "\n",
        "directories = {\n",
        "    'generated_images': 'generated_images',  # For generated samples\n",
        "    'reconstructions': 'reconstruction_images',  # For reconstruction comparisons\n",
        "    'checkpoints': 'model_checkpoints',  # For saved model states\n",
        "    'metrics': 'training_metrics'  # For loss plots and other metrics\n",
        "}\n",
        "\n",
        "for dir_name in directories.values():\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.makedirs(dir_name)\n",
        "        print(f\"Created directory: {dir_name}\")\n",
        "\n",
        "# Training setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ImprovedVAE().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_of_epochs, eta_min=1e-6)\n",
        "\n",
        "print(\"Parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "best_loss = float('inf')\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# Training loop\n",
        "print(\"Starting training...\")\n",
        "best_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_of_epochs):\n",
        "    # Adjust KL weight using cyclic schedule\n",
        "    kld_weight = 0.001 * (1 + np.sin(np.pi * epoch / 10))\n",
        "    \n",
        "    # Train for one epoch\n",
        "    train_loss = train_epoch(model, optimizer, train_iterator, device, kld_weight)\n",
        "    \n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    # Evaluate and visualize after each epoch\n",
        "    test_loss = evaluate(model, test_iterator, device)\n",
        "    test_losses.append(test_loss)\n",
        "    \n",
        "    # Get a batch of images for visualization\n",
        "    images, _ = next(test_iterator)\n",
        "    images = images.to(device)\n",
        "    \n",
        "    # Generate and save visualizations\n",
        "    visualize_results(model, images, epoch, device, directories)\n",
        "    \n",
        "    # Print progress with more detailed information\n",
        "    print(f'Epoch [{epoch:04d}/{num_of_epochs:04d}]')\n",
        "    print(f'Training Loss: {train_loss:.6f}')\n",
        "    print(f'Test Loss: {test_loss:.6f}')\n",
        "    print(f'KLD Weight: {kld_weight:.6f}')\n",
        "    print('-' * 50)\n",
        "    \n",
        "    # Save best model with epoch number in filename\n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        model_path = os.path.join(directories['checkpoints'], f'best_vae_epoch_{epoch:04d}.pth')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'test_loss': test_loss,\n",
        "            'best_loss': best_loss\n",
        "        }, model_path)\n",
        "        print(f'Saved new best model at epoch {epoch} with test loss: {test_loss:.6f}')\n",
        "    \n",
        "    # Plot and save loss curves\n",
        "    if epoch % 5 == 0:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(train_losses, label='Training Loss', color='blue', alpha=0.7)\n",
        "        plt.plot(test_losses, label='Test Loss', color='red', alpha=0.7)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(f'Training Progress - Epoch {epoch}')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(directories['metrics'], f'loss_plot_epoch_{epoch:04d}.png'))\n",
        "        plt.close()\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew-Ik2p6pIkW"
      },
      "source": [
        "**Latent interpolations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "Sbb3a--HkGzZ",
        "outputId": "f8d8cbcd-f052-48a7-997e-9b44f42695ab"
      },
      "outputs": [],
      "source": [
        "# now show some interpolations (note you do not have to do linear interpolations as shown here, you can do non-linear or gradient-based interpolation if you wish)\n",
        "col_size = int(np.sqrt(batch_size))\n",
        "\n",
        "z0 = z[0:col_size].repeat(col_size,1) # z for top row\n",
        "z1 = z[batch_size-col_size:].repeat(col_size,1) # z for bottom row\n",
        "\n",
        "t = torch.linspace(0,1,col_size).unsqueeze(1).repeat(1,col_size).view(batch_size,1).to(device)\n",
        "\n",
        "lerp_z = (1-t)*z0 + t*z1 # linearly interpolate between two points in the latent space\n",
        "lerp_g = Generator.sample(lerp_z) # sample the model at the resulting interpolated latents\n",
        "\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.grid(False)\n",
        "plt.imshow(torchvision.utils.make_grid(lerp_g).cpu().numpy().transpose(1, 2, 0), cmap=plt.cm.binary)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Vxe_qIbRXS"
      },
      "source": [
        "**FID scores**\n",
        "\n",
        "Evaluate the FID from 10k of your model samples (do not sample more than this) and compare it against the 10k test images. Calculating FID is somewhat involved, so we use a library for it. It can take a few minutes to evaluate. Lower FID scores are better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4RmRO6U7mLbq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install clean-fid\n",
        "import os\n",
        "from cleanfid import fid\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "canqHlS7bRXT"
      },
      "outputs": [],
      "source": [
        "# define directories\n",
        "real_images_dir = 'real_images'\n",
        "generated_images_dir = 'generated_images'\n",
        "num_samples = 10000 # do not change\n",
        "\n",
        "# create/clean the directories\n",
        "def setup_directory(directory):\n",
        "    if os.path.exists(directory):\n",
        "        !rm -r {directory} # remove any existing (old) data\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# setup_directory(real_images_dir)\n",
        "# setup_directory(generated_images_dir)\n",
        "\n",
        "# generate and save 10k model samples\n",
        "num_generated = 0\n",
        "while num_generated < num_samples:\n",
        "\n",
        "    # sample from your model, you can modify this\n",
        "    z = torch.randn(batch_size, latent_dim).to(device)\n",
        "    samples_batch = N.sample(z).cpu().detach()\n",
        "\n",
        "    for image in samples_batch:\n",
        "        if num_generated >= num_samples:\n",
        "            break\n",
        "        save_image(image, os.path.join(generated_images_dir, f\"gen_img_{num_generated}.png\"))\n",
        "        num_generated += 1\n",
        "\n",
        "# save 10k images from the CIFAR-100 test dataset\n",
        "num_saved_real = 0\n",
        "while num_saved_real < num_samples:\n",
        "    real_samples_batch, _ = next(test_iterator)\n",
        "    for image in real_samples_batch:\n",
        "        if num_saved_real >= num_samples:\n",
        "            break\n",
        "        save_image(image, os.path.join(real_images_dir, f\"real_img_{num_saved_real}.png\"))\n",
        "        num_saved_real += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RVOmtzabRXT",
        "outputId": "33d56f24-a300-4a81-e8f7-cfaedc180411"
      },
      "outputs": [],
      "source": [
        "# compute FID\n",
        "score = fid.compute_fid(real_images_dir, generated_images_dir, mode=\"clean\")\n",
        "print(f\"FID score: {score}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
