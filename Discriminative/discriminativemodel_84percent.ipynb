{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N22Uz-kLiZW"
   },
   "source": [
    "### Main imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MK1Jl7nkLnPA",
    "outputId": "140fac74-15b2-4d66-ea7c-ea799d544617"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import numpy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as disp\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print('Imports complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run1dh_hM0oO"
   },
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IAdqSy7nIFLq",
    "outputId": "09795199-1a1c-4767-b1e6-558c7f407755"
   },
   "outputs": [],
   "source": [
    "# # URL of the file\n",
    "# url = 'https://data.ncl.ac.uk/ndownloader/articles/24574354/versions/1'\n",
    "\n",
    "# # make directory if it doesn't exist\n",
    "# directory = 'classification-data'\n",
    "# if not os.path.exists(directory):\n",
    "#     os.makedirs(directory)\n",
    "\n",
    "# # where to save the downloaded file\n",
    "# file_path = os.path.join(directory, 'downloaded_file.zip')\n",
    "\n",
    "# # send a GET request to the URL with stream=True for chunked download\n",
    "# response = requests.get(url, stream=True)\n",
    "\n",
    "# # check if the request was successful\n",
    "# if response.status_code == 200:\n",
    "#     # total size in bytes from the header\n",
    "#     total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "#     # progress bar\n",
    "#     with open(file_path, 'wb') as file, tqdm(\n",
    "#             desc=\"Downloading\",\n",
    "#             total=total_size,\n",
    "#             unit='B',\n",
    "#             unit_scale=True,\n",
    "#             unit_divisor=1024,\n",
    "#     ) as progress_bar:\n",
    "    \n",
    "#         for chunk in response.iter_content(chunk_size=1024):\n",
    "#             if chunk:\n",
    "#                 file.write(chunk)\n",
    "#                 progress_bar.update(len(chunk))\n",
    "\n",
    "#     print(\"File downloaded successfully!\")\n",
    "\n",
    "#     # unzipping\n",
    "#     with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "#         zip_ref.extractall(directory)\n",
    "#         print(f\"Files unzipped successfully in the '{directory}' directory!\")\n",
    "\n",
    "#     # remove the zip file after extraction\n",
    "#     os.remove(file_path)\n",
    "#     print(\"Zip file removed after extraction.\")\n",
    "\n",
    "# else:\n",
    "#     print(f\"Failed to download the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3sLAPCRM6_h"
   },
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bK383zeDM4Ac",
    "outputId": "b6597294-6f1d-4372-8c51-f08072bc78ab"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# directory where the data is stored\n",
    "data_dir = 'classification-data'\n",
    "\n",
    "# the .npy files\n",
    "train_x_path = os.path.join(data_dir, 'train_x.npy')\n",
    "train_y_path = os.path.join(data_dir, 'train_y.npy')\n",
    "valid_x_path = os.path.join(data_dir, 'valid_x.npy')\n",
    "valid_y_path = os.path.join(data_dir, 'valid_y.npy')\n",
    "test_x_path = os.path.join(data_dir, 'test_x.npy')\n",
    "test_y_path = os.path.join(data_dir, 'test_y.npy')\n",
    "\n",
    "# load the data\n",
    "print(\"Loading data...\")\n",
    "train_x = numpy.load(train_x_path)\n",
    "train_y = numpy.load(train_y_path)\n",
    "valid_x = numpy.load(valid_x_path)\n",
    "valid_y = numpy.load(valid_y_path)\n",
    "test_x = numpy.load(test_x_path)\n",
    "test_y = numpy.load(test_y_path)\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "print(\"Shape of one training sample:\", train_x.shape[1:])\n",
    "print(\"Shape of one validation sample:\", valid_x.shape[1:])\n",
    "print(\"Shape of one test sample:\", test_x.shape[1:])\n",
    "\n",
    "train_x = train_x.reshape(-1, 3, 28, 28) \n",
    "valid_x = valid_x.reshape(-1, 3, 28, 28) \n",
    "test_x = test_x.reshape(-1, 3, 28, 28)\n",
    "\n",
    "print(\"Shape of one training sample:\", train_x.shape[1:])\n",
    "print(\"Shape of one validation sample:\", valid_x.shape[1:])\n",
    "print(\"Shape of one test sample:\", test_x.shape[1:])\n",
    "\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     # resize all images to 128x128\n",
    "#     # random flipping\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     # all images are RGB\n",
    "#     # transforms.Lambda(lambda x: x.convert(\"RGB\")),\n",
    "#     # randomly rotate images up to 45 degrees\n",
    "#     transforms.RandomRotation(5),\n",
    "#     # color jitter\n",
    "#     transforms.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8, hue=0.5),\n",
    "#     # affine transformations with shear\n",
    "#     transforms.RandomAffine(degrees=30, shear=15),\n",
    "#     # convert some images to grayscale\n",
    "#     transforms.RandomGrayscale(p=0.3),\n",
    "#     # Gaussian blur\n",
    "#     transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),\n",
    "#     # perspective transformation\n",
    "#     transforms.RandomPerspective(distortion_scale=0.5, p=0.2),\n",
    "#     # randomly invert colors\n",
    "#     transforms.RandomInvert(p=0.1),\n",
    "#     # adjust contrast\n",
    "#     transforms.RandomAutocontrast(p=0.3),\n",
    "#     # randomly adjust sharpness\n",
    "#     transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.2),\n",
    "#     transforms.Resize((28, 28)),\n",
    "#     # convert to tensor\n",
    "#     transforms.ToTensor(),\n",
    "#     # randomly erase parts of the image\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.2), ratio=(0.3, 3.3)),\n",
    "# ])\n",
    "\n",
    "# # for the test set, only resize, convert to tensor\n",
    "# test_transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.Resize((28, 28)),\n",
    "#     # transforms.Lambda(lambda x: x.convert(\"RGB\")),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# train_x = torch.stack([train_transform(img) for img in train_x])\n",
    "# valid_x = torch.stack([test_transform(img) for img in valid_x])\n",
    "# test_x = torch.stack([test_transform(img) for img in test_x])\n",
    "\n",
    "# train_x = torch.stack([train_transform(img) for img in torch.tensor(train_x, dtype=torch.float32)])\n",
    "# valid_x = torch.stack([test_transform(img) for img in torch.tensor(valid_x, dtype=torch.float32)])\n",
    "# test_x = torch.stack([test_transform(img) for img in torch.tensor(test_x, dtype=torch.float32)])\n",
    "\n",
    "# convert NumPy arrays to PyTorch tensors\n",
    "train_x = torch.from_numpy(train_x).float()\n",
    "train_y = torch.from_numpy(train_y).long()\n",
    "valid_x = torch.from_numpy(valid_x).float()\n",
    "\n",
    "valid_y = torch.from_numpy(valid_y).long()\n",
    "test_x = torch.from_numpy(test_x).float()\n",
    "test_y = torch.from_numpy(test_y).long()\n",
    "\n",
    "# create TensorDataset instances\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "valid_dataset = TensorDataset(valid_x, valid_y)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "\n",
    "# batch size\n",
    "batch_size = 64\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "# Helper function to cycle through the data indefinitely\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "# iterators for training, validation, and test loaders\n",
    "train_iterator = iter(cycle(train_loader))\n",
    "valid_iterator = iter(cycle(valid_loader))\n",
    "test_iterator = iter(cycle(test_loader))\n",
    "\n",
    "# class names\n",
    "class_names = [str(i) for i in range(20)]\n",
    "\n",
    "# dataset sizes\n",
    "print(f'> Size of training dataset: {len(train_dataset)}')\n",
    "print(f'> Size of validation dataset: {len(valid_dataset)}')\n",
    "print(f'> Size of test dataset: {len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-FdW5HnimG2"
   },
   "source": [
    "**View some of the test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "id": "BtJs-qxHRLXz",
    "outputId": "9e93788b-f953-48a6-b4b4-a64eb385563c"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 70\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    img = test_loader.dataset[i][0].numpy().transpose(1, 2, 0)\n",
    "    img = numpy.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.xlabel(class_names[test_loader.dataset[i][1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1 - 80ish% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce_loss(inputs, targets)\n",
    "        p_t = torch.exp(-ce_loss)\n",
    "        loss = self.alpha * (1 - p_t) ** self.gamma * ce_loss\n",
    "        return loss\n",
    "\n",
    "class EnhancedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16,kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.fc1 = nn.Linear(64, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.max_pool2d(x, 2) \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.bn2(x) \n",
    "        x = F.max_pool2d(x, 2) \n",
    "        x = F.relu(self.conv3(x)) \n",
    "        x = self.bn3(x)\n",
    "        x = F.max_pool2d(x, 2) \n",
    "        x = F.relu(self.conv4(x)) \n",
    "        x = self.bn4(x)\n",
    "        x = F.max_pool2d(x, 2) \n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        # return x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "# N = EnhancedCNN().to(device)\n",
    "\n",
    "# print(f'> Number of parameters {len(torch.nn.utils.parameters_to_vector(N.parameters()))}')\n",
    "\n",
    "# criterion = FocalLoss(alpha=0.1, gamma=1)\n",
    "\n",
    "# optimiser = torch.optim.AdamW(N.parameters(), lr=0.001, weight_decay=0.02)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', factor=0.2, patience=7)\n",
    "\n",
    "# plot_data = []\n",
    "# steps = 0\n",
    "# best_test_acc = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1UBl0PJjY-f"
   },
   "source": [
    "### Main training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "kb5909Y8D_zx",
    "outputId": "fe198374-76f9-486e-e30c-c5ea42080981"
   },
   "outputs": [],
   "source": [
    "# keep within our optimisation step budget\n",
    "def grid_search(alpha, gamma, lr, weight_decay, factor, patience):\n",
    "    steps = 0\n",
    "    best_test_acc = 0.0\n",
    "    best_train_acc = 0.0\n",
    "\n",
    "    N = EnhancedCNN().to(device)\n",
    "\n",
    "    print(f'> Number of parameters {len(torch.nn.utils.parameters_to_vector(N.parameters()))}')\n",
    "\n",
    "    criterion = FocalLoss(alpha, gamma)\n",
    "\n",
    "    optimiser = torch.optim.AdamW(N.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', factor=factor, patience=patience)\n",
    "\n",
    "    while (steps < 10000):\n",
    "\n",
    "        # arrays for metrics\n",
    "        train_loss_arr = numpy.zeros(0)\n",
    "        train_acc_arr = numpy.zeros(0)\n",
    "        test_acc_arr = numpy.zeros(0)\n",
    "\n",
    "        # iterate through some of the train dateset\n",
    "        for i in range(1000):\n",
    "            x,t = next(train_iterator)\n",
    "            x,t = x.to(device), t.to(device)\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            p = N(x)\n",
    "            pred = p.argmax(dim=1, keepdim=True)\n",
    "            loss = criterion(p, t)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            steps += 1\n",
    "\n",
    "            train_loss_arr = numpy.append(train_loss_arr, loss.cpu().data)\n",
    "            train_acc_arr = numpy.append(train_acc_arr, pred.data.eq(t.view_as(pred)).float().mean().item())\n",
    "\n",
    "        # iterate over the entire test dataset\n",
    "        for x,t in test_loader:\n",
    "            x,t = x.to(device), t.to(device)\n",
    "            p = N(x)\n",
    "            loss = criterion(p,t)\n",
    "            pred = p.argmax(dim=1, keepdim=True)\n",
    "            test_acc_arr = numpy.append(test_acc_arr, pred.data.eq(t.view_as(pred)).float().mean().item())\n",
    "\n",
    "        avg_train_loss = numpy.mean(train_loss_arr)\n",
    "        avg_train_acc = numpy.mean(train_acc_arr)\n",
    "        avg_test_acc = numpy.mean(test_acc_arr)\n",
    "\n",
    "        scheduler.step(avg_test_acc)\n",
    "\n",
    "        final_train_acc = numpy.mean(train_acc_arr)\n",
    "        final_test_acc = avg_test_acc\n",
    "\n",
    "        if final_test_acc > best_test_acc:\n",
    "            best_test_acc = final_test_acc\n",
    "            best_train_acc = final_train_acc\n",
    "\n",
    "        if avg_test_acc > best_test_acc:\n",
    "            best_test_acc = avg_test_acc\n",
    "            # torch.save(N.state_dict(), 'best_model.pth')\n",
    "\n",
    "        # print your loss and accuracy data - include this in the final report\n",
    "        # print('steps: {:.2f}, train loss: {:.3f}, train acc: {:.3f}±{:.3f}, test acc: {:.3f}±{:.3f}'.format(\n",
    "        #     steps, train_loss_arr.mean(),train_acc_arr.mean(),train_acc_arr.std(),test_acc_arr.mean(),test_acc_arr.std()))\n",
    "\n",
    "        # # plot your accuracy graph - add a graph like this in your final report\n",
    "        # plot_data.append([steps, numpy.array(train_acc_arr).mean(), numpy.array(train_acc_arr).std(), numpy.array(test_acc_arr).mean(), numpy.array(test_acc_arr).std()])\n",
    "        # reward_list = []\n",
    "        # plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey', label=\"Train accuracy\")\n",
    "        # plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
    "        # plt.plot([x[0] for x in plot_data], [x[3] for x in plot_data], '-', color='tab:purple', label=\"Test accuracy\")\n",
    "        # plt.fill_between([x[0] for x in plot_data], [x[3]-x[4] for x in plot_data], [x[3]+x[4] for x in plot_data], alpha=0.2, color='tab:purple')\n",
    "        # plt.xlabel('Steps')\n",
    "        # plt.ylabel('Accuracy')\n",
    "        # plt.legend(loc=\"upper left\")\n",
    "        # plt.show()\n",
    "        # disp.clear_output(wait=True)\n",
    "\n",
    "    print(\"Model train accuracy: \", best_train_acc)\n",
    "    print(\"Model test accuracy: \", best_test_acc)\n",
    "\n",
    "\n",
    "    return best_test_acc, best_train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = [0.05, 0.1, 0.2]\n",
    "gamma_values = [1, 2] # didnt try 3 and 5\n",
    "lr_values = [0.001, 0.0001]\n",
    "weight_decay_values = [0.001, 0.02]\n",
    "factor_values = [0.1, 0.2, 0.3]\n",
    "patience_values = [5, 7]\n",
    "\n",
    "best_acc = 0.0\n",
    "best_poarams = None\n",
    "\n",
    "# for alpha, gamma, lr, wd, factor, patience in more_itertools.product(\n",
    "#     alpha_values, gamma_values, lr_values, weight_decay_values, factor_values, patience_values):\n",
    "    \n",
    "#     final_train_acc, final_test_acc = grid_search(alpha, gamma, lr, wd, factor, patience) # THIS WILL TAKE A WHILE\n",
    "#     if final_test_acc > best_acc:\n",
    "#         best_acc = final_test_acc\n",
    "#         best_params = (alpha, gamma, lr, wd, factor, patience)\n",
    "\n",
    "model_number = 1\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for gamma in gamma_values:\n",
    "        for lr in lr_values:\n",
    "            for wd in weight_decay_values:\n",
    "                for factor in factor_values:\n",
    "                    for patience in patience_values:\n",
    "                        final_train_acc, final_test_acc = grid_search(alpha, gamma, lr, wd, factor, patience)\n",
    "                        if final_test_acc > best_acc:\n",
    "                            best_acc = final_test_acc\n",
    "                            best_params = (alpha, gamma, lr, wd, factor, patience)\n",
    "                        print(\"Parameters: \", alpha, gamma, lr, wd, factor, patience)\n",
    "                        print(f'Model {model_number} complete.')\n",
    "                        model_number += 1\n",
    "\n",
    "print(f'Best test accuracy: {best_acc}')\n",
    "print(f'Best parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTFqiEHzMOVw"
   },
   "source": [
    "## Inference on data\n",
    "\n",
    "This is useful for analysis but is entirely optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "id": "kTzXmAdQK2c2",
    "outputId": "447feb45-9891-4a66-bdce-36e3830c2b0e"
   },
   "outputs": [],
   "source": [
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
    "    img = numpy.clip(img, 0, 1)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    img = numpy.transpose(img, (1, 2, 0))\n",
    "    plt.imshow(img)\n",
    "\n",
    "    predicted_label = numpy.argmax(predictions_array)\n",
    "    color = '#335599' if predicted_label == true_label else '#ee4433'\n",
    "\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                  100*numpy.max(predictions_array),\n",
    "                                  class_names[true_label]),\n",
    "                                  color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array[i], true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(20), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = numpy.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('#ee4433')\n",
    "    thisplot[true_label].set_color('#335599')\n",
    "\n",
    "test_images, test_labels = next(test_iterator)\n",
    "test_images, test_labels = test_images.to(device), test_labels.to(device)\n",
    "test_preds = torch.softmax(N(test_images), dim=1).data.cpu().numpy()\n",
    "num_rows = 8\n",
    "num_cols = 4\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "    plot_image(i, test_preds, test_labels.cpu().numpy(), test_images.cpu().numpy()) # Used .numpy() here\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_value_array(i, test_preds, test_labels.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
